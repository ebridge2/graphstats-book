\chapter{Learning representations theory}
\label{app:ch13}


In the main text, you learned many approaches for representing networks. These methods, while in general rather principled, also have substantial mathematical foundations as to why they are sensible. In particular, when you make assumptions about the network sample that you have, you can in general *prove* that the approaches we described in the main text provide reasonable algorithmic approaches. We outline more in-depth discussions on the sections here:

\begin{enumerate}
    \item Section \ref{app:ch13:mle} discusses maximum likelihood approaches to ER and SBM networks, and the limitations as to why we resort to spectral approaches for RDPGs.
    \item Section \ref{app:ch13:spectral} discusses consistent estimators of latent position matrices and joint matrices.
\end{enumerate}

\input{appendix/ch13/mle}
\input{appendix/ch13/ase}
\input{appendix/ch13/spectral}


\bibliographystyle{vancouver}
\bibliography{references}