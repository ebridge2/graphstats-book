\section{Closing thoughts, and a path ahead}
\label{sec:ch10:next}

At this point, the story of network machine learning is far from over. Hopefully, by now, you have a general idea of how to visualize, conceptualize, and leverage a number of strategies for networks when you come across them in your your own work. Unfortunately, our journey with you, for now, is coming to an end. Before we let you go, we want to leave you with a route forward. We tried our best to give you an introduction to network machine learning without going too down in the weeds, so that you can keep the problem space at the top of your minds. As you peel away the layers and go further in network machine learning, you might find that things can get complicated pretty quickly. As we explained back in the Preface, network machine learning is an inherently intersectional field, which means that for every layer that you peel back, you might have to push forward in a few areas of expertise simultaneously. In particular, these areas of expertise are, in our opinion, in decreasing order of necessity:

\paragraph{Probability and statistics}
Probability and statistics will help you to better contextualize random network models and estimators of parameters of network models. Particular areas of focus should be on the concept of random variables, properties of random variables (such as the expectation and the variance), estimators, and properties of estimators (unbiasedness and consistency). 

\paragraph{Applied linear algebra}
Applied linear algebra will help you to better understand many of the procedures and algorithms that you come across in network analyses, and adapt new techniques to matrix representations of networks. Particular areas of focus should be on matrix decompositions, techniques for obtaining matrix decompositions, and performing algebraic operations with matrices. Focus should be placed on randomized approaches for making these computations, which can often make slight tradeoffs for precision with radical improvements in speed and scalability. These tradeoffs can often be worthwhile, particularly in large and sparse networks with high numbers of nodes but comparatively small numbers of edges.

\paragraph{Applied machine learning}

As you have learned by now in this book, many network learning techniques proceed by effectively ``adapting'' the network (through, say, an explicit or implicit representation) to a modality which can be used with other machine learning algorithms. Network machine learning is a rapidly evolving field of interest, and many of the most revolutionary approaches that we have discussed in this book came from techniques originally developed for non-network data. With further expertise on machine learning, we believe that you will be able to continue to build on this foundation to develop your own solutions to novel network machine learning problems. 


\paragraph{Real analysis and advanced probability theory}

Real analysis and advanced probability theory will give you the ability to better appreciate the theoretical situations when network analysis approaches will work, when they will not work, and will allow you to contribute new ideas to network learning theory. Particular areas of focus should be on sequences of random variables, limits of sequences of random variables, asymptotic behaviors of sequences of random variables, and concentration inequalities, which are helpful for rigorously defining and proving when your approaches produce desirable results. 

We hope that our book will serve as a catalyst for you to continue branching out and learning more on network machine learning, and that you will continue your evolution as a network science expert. If you would like to contact the authors for any reason, please feel free to reach out to \texttt{ericwb95@gmail.com}.