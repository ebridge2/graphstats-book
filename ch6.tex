\chapter{Learning Network Representations}
\label{sec:ch6}

Network-valued data differs from more traditional representations of data. Networks have a particularly useful topological structure: the manner in which the edges and nodes manifest gives useful information about relationships between the nodes. However, this structure comes with a drawback: the vast majority of machine learning, to date, is designed for tabular structures living in a space of features, where each data point is defined by a vector of scalar feature values. In this way of viewing data, each observation tends to be treated roughly independently (although possible relationships like correlations can still be found under the hood).

Even modern neural networks operate using this type of data. The entire field of \textit{representation learning}, a subfield of deep learning, uses dimensionality reduction techniques to bring data -- often images or text -- into a tabular space of features. Almost all well-known neural network architectures turn data into feature representations under the hood.

In fact, pretty much every other branch of machine learning and data science project data into some space where each datapoint can be viewed as a literal point in space, in which the axes correspond to properties (the features) of that datapoint. Representing data in this way is powerful and so it would be useful for us to be able to fluidly move between graph-space and feature-representation space.

In network-valued data, with $n$ nodes, there are $n!$ possible dependencies that could exist in the data (that is, each node could be dependent on the other nodes, which in turn are dependent on other nodes, so on and so forth, for all of the nodes in the network). To make this a bit more tractable to learn from, in network learning, we tend to serialize the network using one of the types of models which we learned about in Section \ref{sec:ch4:net-rep}. For network learning, the most common approach that we will use is the bag of edges approach in Section \ref{sec:ch4:net-rep:bagofnodes} approach. The idea is to turn each node into a data point living in feature-representation space.

There are a variety of ways we can do this, the most common of which tend to be the various types of spectral embeddings. When we have multiple networks, we will use a combination of the bag of nodes approach in Section \ref{sec:ch4:net-rep:bagofnodes} and the bag of networks approach in Section \ref{sec:ch4:net-rep:bagofnets}. In Figure \ref{fig:ch6:netrep}, one step is "choosing a suitable representation": that is what this chapter is all about.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{representations/ch6/Images/network_reps.png}
    \caption[Representation learning schematic]{The statistical learning pipeline. In this chapter, you will learn about how to represent networks you observe in your analysis.}
    \label{fig:ch6:netrep}
\end{figure}

It is important to note that to learn and derive useful insights from these representations, you really don't need to understand anything about the last chapter at {all}, in general. You can take the representations you learn about, apply machine learning systems to them, and then study the networks in isolation. However, these representations tend to make more intuitive sense in the context of Section \ref{sec:ch5}, and further, Section \ref{sec:ch5} provides context and assumptions in which these representations directly motivate many of the applications you will learn in Chapter \ref{sec:ch7} onwards. So, we'd recommend learning this chapter somewhat in tandem with the preceding section: as you learn a new algorithm for finding a representation of a network, go back and think about how that representation is motivated by a statistical model. This will help you understand how the pieces start to fit together as you get to the later chapters in the network machine learning landscape.

Having a complete understanding of the results in this section, and particularly, the justification for {why} we approach network representation problems in the way in which we do requires some mathematical background. For this reason, we have prepared the supplementary section Appendix \ref{app:ch13} for more advanced readers.


\input{representations/ch6/mle}
\input{representations/ch6/why-embed}
\input{representations/ch6/ase}
\input{representations/ch6/lse}
\input{representations/ch6/multigraph}
\input{representations/ch6/jointrep}
\input{representations/ch6/dimest}

\bibliographystyle{vancouver}
\bibliography{references}