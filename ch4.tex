\chapter{Mathematical properties of network data}
\label{sec:ch4}


In this chapter we present the basics of network-valued data. As we said in the introduction, for many reasons, network-valued data is fundamentally different from other forms of data, such as tabular data, that are often used in machine learning. In Section \ref{sec:ch1:challenges}, we introduced a figure which is going to come up repeatedly throughout this book, so that we can contextualize where we are in the process of learning {how to learn} from networks. Figure \ref{fig:ch4:rep_properties} shows the next step: describing the network you observe in  your analysis.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{representations/ch4/Images/network_reps.png}
    \caption[Describing networks schematic]{The statistical learning pipeline.}
    \label{fig:ch4:rep_properties}
\end{figure}

When we try to learn from a network, the first thing to do is to describe that network. The network that we see is known as a \emph{sample} or an \emph{observation} of a network. As we described in \ref{sec:ch1:challenges}, we call it this is because the network we observe is only a noisy observation of what we are trying to study. We need good ways to represent this noisy observation.

Throughout the subsections, we will learn about the fundamental representation we use for networks in this book, which is called the adjacency matrix.

This section will tie in directly with Section \ref{sec:ch4:prop-net}, so be aware that many of these concepts will be addressed in much more depth as the book progresses.

\newpage 

\input{representations/ch4/matrix-representations}
\input{representations/ch4/properties-of-networks}
\input{representations/ch4/network-representations}
\input{representations/ch4/regularization}

\bibliographystyle{vancouver}
\bibliography{references}